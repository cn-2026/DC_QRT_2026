{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0edeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\alexb\\Documents\\EI3\\APST1\\Data Challenge\\project_root\n",
      "Looking for data at: c:\\Users\\alexb\\Documents\\EI3\\APST1\\Data Challenge\\project_root\\data\\raw\\train\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load the tables\n",
    "# -------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "project_root = None\n",
    "\n",
    "# Fallback: search for the data/raw/train directory structure\n",
    "if project_root is None:\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if (p / 'data' / 'raw' / 'train').is_dir():\n",
    "            project_root = p\n",
    "            break\n",
    "\n",
    "if project_root is None:\n",
    "    project_root = cwd\n",
    "\n",
    "data_raw_train = project_root / 'data' / 'raw' / 'train'\n",
    "clinical_path = data_raw_train / 'clinical_train.csv'\n",
    "molecular_path = data_raw_train / 'molecular_train.csv'\n",
    "target_path = data_raw_train / 'target_train.csv'\n",
    "code\n",
    "python\n",
    "# -------------------------\n",
    "# 6.5. Hyperparameter optimization (Optuna) with K-Fold IPCW objective\n",
    "# - Uses K-Fold cross-validation on the training split and maximizes IPCW C-index\n",
    "# - After tuning, computes feature importances, selects top features, and retrains\n",
    "# -------------------------\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "import numpy as np\n",
    "\n",
    "# K-fold for inner CV during optimization\n",
    "N_SPLITS = 5\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 6),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # Iterate folds on the training split (X_train/y_train defined earlier)\n",
    "    for train_idx, val_idx in kf.split(range(len(X_train))):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        y_tr = y_train[train_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "\n",
    "        # Build model for this trial\n",
    "        gb = GradientBoostingSurvivalAnalysis(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_depth=params['max_depth'],\n",
    "            min_samples_split=params['min_samples_split'],\n",
    "            min_samples_leaf=params['min_samples_leaf'],\n",
    "            subsample=params['subsample'],\n",
    "            max_features=params['max_features'],\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        pipe = Pipeline([(\"pre\", preprocess), (\"model\", gb)])\n",
    "        # Fit on fold train\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        # Predict risk on validation fold\n",
    "        pred_val = pipe.predict(X_val)\n",
    "        # IPCW measured on this fold (train used to estimate censoring weights)\n",
    "        try:\n",
    "            ipcw_fold = concordance_index_ipcw(y_tr, y_val, pred_val)[0]\n",
    "        except Exception:\n",
    "            ipcw_fold = 0.0\n",
    "        scores.append(ipcw_fold)\n",
    "\n",
    "    # Return mean IPCW across folds\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "# Create and run study (use a moderate number of trials to start)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=40, n_jobs=1)\n",
    "\n",
    "print(\"Best IPCW (CV):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "# Save best params into best_params variable for downstream cells\n",
    "best_params = study.best_params\n",
    "\n",
    "# -------------------------\n",
    "# Train final pipeline on full X_train using best params\n",
    "# -------------------------\n",
    "gb_final = GradientBoostingSurvivalAnalysis(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    subsample=best_params['subsample'],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=42\n",
    ")\n",
    "pipe_full = Pipeline([('pre', preprocess), ('model', gb_final)])\n",
    "pipe_full.fit(X_train, y_train)\n",
    "\n",
    "# Extract fitted preprocessor and model\n",
    "pre = pipe_full.named_steps['pre']\n",
    "model_fitted = pipe_full.named_steps['model']\n",
    "\n",
    "# Get transformed feature names (best-effort) and importances\n",
    "def get_feature_names(preprocessor, X_ref):\n",
    "    # Try sklearn's get_feature_names_out, fall back to building names\n",
    "    try:\n",
    "        return preprocessor.get_feature_names_out()\n",
    "    except Exception:\n",
    "        names = []\n",
    "        # categorical columns used earlier\n",
    "        categorical_cols = [c for c in ['CENTER','CYTOGENETICS'] if c in X_ref.columns]\n",
    "        numeric_cols = [c for c in X_ref.columns if c not in categorical_cols]\n",
    "        if 'cat' in preprocessor.named_transformers_:\n",
    "            ohe = preprocessor.named_transformers_['cat']\n",
    "            try:\n",
    "                cat_names = ohe.get_feature_names_out(categorical_cols)\n",
    "            except Exception:\n",
    "                cat_names = []\n",
    "                for i, col in enumerate(categorical_cols):\n",
    "                    cats = ohe.categories_[i]\n",
    "                    cat_names.extend([f\"{col}_%s\" % str(x) for x in cats])\n",
    "            names.extend(list(cat_names))\n",
    "        names.extend(numeric_cols)\n",
    "        return names\n",
    "\n",
    "feature_names = get_feature_names(pre, X_train)\n",
    "importances = model_fitted.feature_importances_\n",
    "\n",
    "# Create a sorted DataFrame of importances\n",
    "import pandas as pd\n",
    "feat_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feat_df = feat_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "print(feat_df.head(30))\n",
    "feat_df.to_csv('feature_importances.csv', index=False)\n",
    "\n",
    "# Select top features (by preprocessed feature importance)\n",
    "TOP_K = min(50, len(feature_names))\n",
    "top_idx = np.argsort(importances)[::-1][:TOP_K]\n",
    "selected_feature_names = [feature_names[i] for i in top_idx]\n",
    "pd.Series(selected_feature_names).to_csv('selected_features_preprocessed.csv', index=False, header=['feature'])\n",
    "print(\"Selected top features (preprocessed):\")\n",
    "print(selected_feature_names[:30])\n",
    "\n",
    "# Transform X_train/X_test with preprocessor and select the top preprocessed columns\n",
    "X_train_arr = pre.transform(X_train)\n",
    "X_test_arr = pre.transform(X_test)\n",
    "# If sparse, convert to dense\n",
    "if hasattr(X_train_arr, 'toarray'):\n",
    "    X_train_arr = X_train_arr.toarray()\n",
    "    X_test_arr = X_test_arr.toarray()\n",
    "\n",
    "X_train_sel = X_train_arr[:, top_idx]\n",
    "X_test_sel = X_test_arr[:, top_idx]\n",
    "\n",
    "# Retrain a survival model on selected transformed features (no preprocess step needed now)\n",
    "gb_sel = GradientBoostingSurvivalAnalysis(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    subsample=best_params['subsample'],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=42\n",
    ")\n",
    "gb_sel.fit(X_train_sel, y_train)\n",
    "pred_sel = gb_sel.predict(X_test_sel)\n",
    "\n",
    "# Evaluate IPCW for the selected-features model\n",
    "c_index_ipcw_selected = concordance_index_ipcw(y_train, y_test, pred_sel)[0]\n",
    "print(\"IPCW (selected features) on test: \", c_index_ipcw_selected)\n",
    "\n",
    "# Save selected feature names (preprocessed) for reference\n",
    "with open('selected_features_preprocessed.txt', 'w') as f:\n",
    "    for s in selected_feature_names:\n",
    "        f.write(s + '\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "           'EFFECT': str, 'VAF': float, 'DEPTH': float})           'REF': str, 'ALT': str, 'GENE': str, 'PROTEIN_CHANGE': str,    dtype={'ID': str, 'CHR': str, 'START': float, 'END': float,           'EFFECT','VAF','DEPTH'],    names=['ID','CHR','START','END','REF','ALT','GENE','PROTEIN_CHANGE',molecular = pd.read_csv(molecular_path, sep=\",\", header=0,           'ANC': float, 'MONOCYTES': float, 'HB': float, 'PLT': float, 'CYTOGENETICS': str})    dtype={'ID': str, 'CENTER': str, 'BM_BLAST': float, 'WBC': float,    names=['ID','CENTER','BM_BLAST','WBC','ANC','MONOCYTES','HB','PLT','CYTOGENETICS'],clinical = pd.read_csv(clinical_path, sep=\",\", header=0,                     dtype={'ID': str, 'OS_YEARS': float, 'OS_STATUS': float})                     names=['ID','OS_YEARS','OS_STATUS'],target = pd.read_csv(target_path, sep=\",\", header=0,    raise FileNotFoundError(f'Could not find molecular file at {molecular_path}')if not molecular_path.exists():    raise FileNotFoundError(f'Could not find clinical file at {clinical_path}')if not clinical_path.exists():    raise FileNotFoundError(f'Could not find target file at {target_path}')if not target_path.exists():print(f\"Looking for data at: {data_raw_train}\")print(f\"Project root: {project_root}\")# best_params variable is already set above# Also keep best_params persisted (useful for later cells)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b529fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (3323, 3)\n",
      "Clinical shape: (3323, 9)\n",
      "Molecular shape: (10935, 11)\n"
     ]
    }
   ],
   "source": [
    "# We check that the data is loaded correctly\n",
    "print(f\"Target shape: {target.shape}\")\n",
    "print(f\"Clinical shape: {clinical.shape}\")\n",
    "print(f\"Molecular shape: {molecular.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c4f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering with mutation count only:\n",
      "  - Only TOTAL_MUTATED_GENES retained\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. Convert molecular table into per-patient mutation features\n",
    "# -------------------------\n",
    "\n",
    "# FLAG: Mutation feature engineering mode\n",
    "#   0 = mutation count only\n",
    "#   1 = lethal mutation flags (statistically significant)\n",
    "#   2 = all mutations as binary indicators\n",
    "mutation_feature_mode = 0\n",
    "\n",
    "# List of genes to exclude from features (manual exclusions)\n",
    "EXCLUDE_GENES = [\"WT1\", \"ZBTB33\"]\n",
    "\n",
    "# Binary indicator for each gene mutated\n",
    "gene_counts = molecular.groupby(['ID', 'GENE']).size().unstack(fill_value=0)\n",
    "\n",
    "# Drop explicitly excluded genes if present\n",
    "for g in EXCLUDE_GENES:\n",
    "    if g in gene_counts.columns:\n",
    "        gene_counts = gene_counts.drop(columns=g)\n",
    "\n",
    "if mutation_feature_mode == 0:\n",
    "    # Simple mutation count per patient: keep only TOTAL_MUTATED_GENES\n",
    "    gene_counts[\"TOTAL_MUTATED_GENES\"] = (gene_counts > 0).sum(axis=1)\n",
    "    # Reduce to single column so no individual gene flags remain\n",
    "    gene_counts = gene_counts[[\"TOTAL_MUTATED_GENES\"]]\n",
    "    print(f\"Feature engineering with mutation count only:\")\n",
    "    print(f\"  - Only TOTAL_MUTATED_GENES retained\")\n",
    "\n",
    "elif mutation_feature_mode == 1:\n",
    "    # Define lethal mutations: top 10 genes by mortality rate from statistically significant mutations\n",
    "    lethal_mutations = [g for g in mutation_df_filtered.head(10).index.tolist() if g not in EXCLUDE_GENES]\n",
    "    print(f\"Lethal mutations selected (statistically significant, p < 0.05) excluding manual list:\")\n",
    "    for gene in lethal_mutations:\n",
    "        row = mutation_df_filtered.loc[gene]\n",
    "        print(f\"  - {gene}: {row['mortality_rate']:.1f}% ({int(row['deaths'])}/{int(row['patient_count'])} patients, p={row['p_value']:.4f})\")\n",
    "    \n",
    "    # Ensure columns for lethal mutations exist and are binary\n",
    "    for gene in lethal_mutations:\n",
    "        if gene in gene_counts.columns:\n",
    "            gene_counts[gene] = (gene_counts[gene] > 0).astype(int)\n",
    "        else:\n",
    "            gene_counts[gene] = 0\n",
    "\n",
    "    # Aggregate all other (non-lethal) genes into OTHER_MUTATIONS_COUNT\n",
    "    existing_cols = [c for c in gene_counts.columns if c not in lethal_mutations]\n",
    "    if existing_cols:\n",
    "        gene_counts[\"OTHER_MUTATIONS_COUNT\"] = gene_counts[existing_cols].sum(axis=1).astype(int)\n",
    "        # Drop the non-lethal individual gene columns\n",
    "        gene_counts = gene_counts.drop(columns=existing_cols)\n",
    "\n",
    "    # Now explicitly keep only lethal flags + OTHER_MUTATIONS_COUNT\n",
    "    final_cols = lethal_mutations.copy()\n",
    "    if 'OTHER_MUTATIONS_COUNT' in gene_counts.columns:\n",
    "        final_cols.append('OTHER_MUTATIONS_COUNT')\n",
    "\n",
    "    # Add TOTAL_MUTATED_GENES as sum of lethal flags + other count\n",
    "    gene_counts[\"TOTAL_MUTATED_GENES\"] = gene_counts[final_cols].sum(axis=1).astype(int)\n",
    "    final_cols.append('TOTAL_MUTATED_GENES')\n",
    "\n",
    "    # Reduce gene_counts to final columns only (enforces no stray gene flags)\n",
    "    gene_counts = gene_counts[final_cols]\n",
    "\n",
    "    print(f\"\\nFeature engineering with lethal mutation flags (strict):\")\n",
    "    print(f\"  - Kept lethal flags: {len(lethal_mutations)}\")\n",
    "    print(f\"  - Kept OTHER_MUTATIONS_COUNT: {'OTHER_MUTATIONS_COUNT' in gene_counts.columns}\")\n",
    "    print(f\"  - Kept TOTAL_MUTATED_GENES: {'TOTAL_MUTATED_GENES' in gene_counts.columns}\")\n",
    "\n",
    "elif mutation_feature_mode == 2:\n",
    "    # Keep all genes as binary indicators\n",
    "    for col in gene_counts.columns:\n",
    "        gene_counts[col] = (gene_counts[col] > 0).astype(int)\n",
    "    \n",
    "    # Adding a column with a total number of mutated genes\n",
    "    gene_counts[\"TOTAL_MUTATED_GENES\"] = (gene_counts > 0).sum(axis=1)\n",
    "    \n",
    "    print(f\"Feature engineering without lethal mutation flags:\")\n",
    "    print(f\"  - All {len(gene_counts.columns) - 1} genes included as binary indicators\")\n",
    "\n",
    "# VAF summary statistics:\n",
    "vaf_stats = molecular.groupby(\"ID\")[\"VAF\"].agg(['mean','max','min']).add_prefix(\"VAF_\")\n",
    "\n",
    "# Combine molecular features\n",
    "mol_features = gene_counts.join(vaf_stats, how=\"left\").fillna(0)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Merge everything into a single training table\n",
    "# -------------------------\n",
    "\n",
    "X = clinical.merge(mol_features, how=\"left\", on=\"ID\").fillna(0)\n",
    "\n",
    "# Remove ID column\n",
    "X = X.set_index(\"ID\")\n",
    "\n",
    "# Drop complex/unwanted clinical columns\n",
    "if 'CYTOGENETICS' in X.columns:\n",
    "    X = X.drop(columns=['CYTOGENETICS'])\n",
    "\n",
    "# Force categorical fields to string (only keep CENTER if present)\n",
    "if 'CENTER' in X.columns:\n",
    "    X[\"CENTER\"] = X[\"CENTER\"].astype(str)\n",
    "\n",
    "# Prepare survival data: need both OS_YEARS (time) and OS_STATUS (event indicator)\n",
    "survival_data = target.set_index(\"ID\").loc[X.index][[\"OS_YEARS\", \"OS_STATUS\"]].copy()\n",
    "survival_data[\"OS_STATUS\"] = survival_data[\"OS_STATUS\"].astype(bool)\n",
    "\n",
    "# Remove patients with missing survival time\n",
    "valid_idx = ~survival_data[\"OS_YEARS\"].isna()\n",
    "X = X[valid_idx]\n",
    "survival_data = survival_data[valid_idx]\n",
    "\n",
    "# Create structured array for survival analysis\n",
    "y = Surv.from_arrays(\n",
    "    event=survival_data[\"OS_STATUS\"].values,\n",
    "    time=survival_data[\"OS_YEARS\"].values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8066f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CENTER  BM_BLAST     WBC   ANC  MONOCYTES    HB    PLT  \\\n",
      "ID                                                               \n",
      "P132697    MSK      14.0    2.80  0.20       0.70   7.6  119.0   \n",
      "P132698    MSK       1.0    7.40  2.40       0.10  11.6   42.0   \n",
      "P116889    MSK      15.0    3.70  2.10       0.10  14.2   81.0   \n",
      "P132699    MSK       1.0    3.90  1.90       0.10   8.9   77.0   \n",
      "P132700    MSK       6.0  128.00  9.70       0.90  11.1  195.0   \n",
      "...        ...       ...     ...   ...        ...   ...    ...   \n",
      "P121826     VU       1.0    2.50  1.02       0.20  10.2   78.0   \n",
      "P121827     VU       1.5    8.10  2.66       0.45  11.3   40.0   \n",
      "P121830     VU       0.0    1.80  0.55       0.29   9.4   86.0   \n",
      "P121853     VU       5.0    1.37  0.37       0.11  11.4  102.0   \n",
      "P121834     VU       0.0    2.70  0.72       0.23   8.2  239.0   \n",
      "\n",
      "         TOTAL_MUTATED_GENES  VAF_mean  VAF_max  VAF_min  \n",
      "ID                                                        \n",
      "P132697                  8.0  0.251578   0.4220   0.0300  \n",
      "P132698                  3.0  0.272867   0.2825   0.2661  \n",
      "P116889                  2.0  0.039333   0.0480   0.0350  \n",
      "P132699                  9.0  0.209227   0.4770   0.0490  \n",
      "P132700                  1.0  0.472100   0.4721   0.4721  \n",
      "...                      ...       ...      ...      ...  \n",
      "P121826                  4.0  0.169000   0.5010   0.0510  \n",
      "P121827                  2.0  0.427500   0.8190   0.0360  \n",
      "P121830                  6.0  0.244650   0.4390   0.0280  \n",
      "P121853                  3.0  0.184250   0.2690   0.0290  \n",
      "P121834                  4.0  0.275220   0.3450   0.0230  \n",
      "\n",
      "[3173 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# We check the integrity of the merged data\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64241475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. Build preprocessing + model pipeline\n",
    "# -------------------------\n",
    "\n",
    "# Categorical columns from clinical features (compute dynamically in case CYTOGENETICS was dropped)\n",
    "categorical_cols = [c for c in [\"CENTER\", \"CYTOGENETICS\"] if c in X.columns]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# GradientBoostingSurvivalAnalysis - survival-aware model that properly handles censoring\n",
    "gb = GradientBoostingSurvivalAnalysis(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"pre\", preprocess),\n",
    "    (\"model\", gb)\n",
    "])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train/test split and training\n",
    "# -------------------------\n",
    "\n",
    "# Split data (must preserve survival structure)\n",
    "idx_train, idx_test = train_test_split(range(len(X)), test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X.iloc[idx_train]\n",
    "X_test = X.iloc[idx_test]\n",
    "y_train = y[idx_train]\n",
    "y_test = y[idx_test]\n",
    "\n",
    "# Fit the survival pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict risk scores (higher = worse survival)\n",
    "pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81954a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index: 0.271486438886932\n",
      "IPCW C-index: 0.6961526084937755\n"
     ]
    }
   ],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "\n",
    "# lifelines expects higher scores => longer survival (predicted time),\n",
    "# so invert risk scores for concordance.\n",
    "c_index = concordance_index(\n",
    "    event_times=y_test['time'],\n",
    "    predicted_scores=-pred,               # <- invert\n",
    "    event_observed=y_test['event'].astype(bool)\n",
    ")\n",
    "print(\"C-index:\", c_index)\n",
    "\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# -------------------------\n",
    "# 6. Evaluating model performance\n",
    "# -------------------------\n",
    "\n",
    "# IPCW C-index accounts for censoring in the evaluation set\n",
    "c_index_ipcw = concordance_index_ipcw(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    pred\n",
    ")[0]   # first entry is the IPCW-c-index\n",
    "\n",
    "print(\"IPCW C-index:\", c_index_ipcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dac80c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Final Model Performance (with optimized hyperparameters):\n",
      "============================================================\n",
      "C-index: 0.2841\n",
      "IPCW C-index: 0.6853\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7. Train Final Model with Best Hyperparameters\n",
    "# -------------------------\n",
    "\n",
    "# Create final model with best parameters (survival-aware)\n",
    "gb_final = GradientBoostingSurvivalAnalysis(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    subsample=best_params['subsample'],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create final pipeline\n",
    "model = Pipeline([\n",
    "    (\"pre\", preprocess),\n",
    "    (\"model\", gb_final)\n",
    "])\n",
    "\n",
    "# Train final model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate final model\n",
    "c_index_final = concordance_index(\n",
    "    event_times=y_test['time'],\n",
    "    predicted_scores=pred,\n",
    "    event_observed=y_test['event'].astype(bool)\n",
    ")\n",
    "\n",
    "c_index_ipcw_final = concordance_index_ipcw(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    pred\n",
    ")[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Final Model Performance (with optimized hyperparameters):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"C-index: {c_index_final:.4f}\")\n",
    "print(f\"IPCW C-index: {c_index_ipcw_final:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 8. Predict survivability score for all patients (using optimized model)\n",
    "# -------------------------\n",
    "\n",
    "survivability_score = model.predict(X)\n",
    "\n",
    "# Attach score to IDs\n",
    "output = pd.DataFrame({\n",
    "    \"ID\": X.index,\n",
    "    \"SURVIVABILITY_SCORE\": survivability_score\n",
    "})\n",
    "\n",
    "print(output.head())\n",
    "\n",
    "# Save to CSV\n",
    "output.to_csv(\"survivability_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
