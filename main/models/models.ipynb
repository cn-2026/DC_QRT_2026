{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0edeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\alexb\\Documents\\EI3\\APST1\\Data Challenge\\project_root\n",
      "Looking for data at: c:\\Users\\alexb\\Documents\\EI3\\APST1\\Data Challenge\\project_root\\data\\raw\\train\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load the tables\n",
    "# -------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "project_root = None\n",
    "\n",
    "# Fallback: search for the data/raw/train directory structure\n",
    "if project_root is None:\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if (p / 'data' / 'raw' / 'train').is_dir():\n",
    "            project_root = p\n",
    "            break\n",
    "\n",
    "if project_root is None:\n",
    "    project_root = cwd\n",
    "\n",
    "data_raw_train = project_root / 'data' / 'raw' / 'train'\n",
    "clinical_path = data_raw_train / 'clinical_train.csv'\n",
    "molecular_path = data_raw_train / 'molecular_train.csv'\n",
    "target_path = data_raw_train / 'target_train.csv'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Looking for data at: {data_raw_train}\")\n",
    "\n",
    "if not target_path.exists():\n",
    "    raise FileNotFoundError(f'Could not find target file at {target_path}')\n",
    "if not clinical_path.exists():\n",
    "    raise FileNotFoundError(f'Could not find clinical file at {clinical_path}')\n",
    "if not molecular_path.exists():\n",
    "    raise FileNotFoundError(f'Could not find molecular file at {molecular_path}')\n",
    "\n",
    "\n",
    "target = pd.read_csv(target_path, sep=\",\", header=0,\n",
    "                     names=['ID','OS_YEARS','OS_STATUS'],\n",
    "                     dtype={'ID': str, 'OS_YEARS': float, 'OS_STATUS': float})\n",
    "\n",
    "clinical = pd.read_csv(clinical_path, sep=\",\", header=0,\n",
    "    names=['ID','CENTER','BM_BLAST','WBC','ANC','MONOCYTES','HB','PLT','CYTOGENETICS'],\n",
    "    dtype={'ID': str, 'CENTER': str, 'BM_BLAST': float, 'WBC': float,\n",
    "           'ANC': float, 'MONOCYTES': float, 'HB': float, 'PLT': float, 'CYTOGENETICS': str})\n",
    "\n",
    "molecular = pd.read_csv(molecular_path, sep=\",\", header=0,\n",
    "    names=['ID','CHR','START','END','REF','ALT','GENE','PROTEIN_CHANGE',\n",
    "           'EFFECT','VAF','DEPTH'],\n",
    "    dtype={'ID': str, 'CHR': str, 'START': float, 'END': float,\n",
    "           'REF': str, 'ALT': str, 'GENE': str, 'PROTEIN_CHANGE': str,\n",
    "           'EFFECT': str, 'VAF': float, 'DEPTH': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b529fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (3323, 3)\n",
      "Clinical shape: (3323, 9)\n",
      "Molecular shape: (10935, 11)\n"
     ]
    }
   ],
   "source": [
    "# We check that the data is loaded correctly\n",
    "print(f\"Target shape: {target.shape}\")\n",
    "print(f\"Clinical shape: {clinical.shape}\")\n",
    "print(f\"Molecular shape: {molecular.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51c4f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering without lethal mutation flags:\n",
      "  - All 122 genes included as binary indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexb\\AppData\\Local\\Temp\\ipykernel_2060\\2630624795.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gene_counts[\"TOTAL_MUTATED_GENES\"] = (gene_counts > 0).sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. Convert molecular table into per-patient mutation features\n",
    "# -------------------------\n",
    "\n",
    "# FLAG: Mutation feature engineering mode\n",
    "#   0 = mutation count only\n",
    "#   1 = lethal mutation flags (statistically significant)\n",
    "#   2 = all mutations as binary indicators\n",
    "mutation_feature_mode = 2\n",
    "\n",
    "# List of genes to exclude from features (manual exclusions)\n",
    "EXCLUDE_GENES = [\"WT1\", \"ZBTB33\"]\n",
    "\n",
    "# Binary indicator for each gene mutated\n",
    "gene_counts = molecular.groupby(['ID', 'GENE']).size().unstack(fill_value=0)\n",
    "\n",
    "# Drop explicitly excluded genes if present\n",
    "for g in EXCLUDE_GENES:\n",
    "    if g in gene_counts.columns:\n",
    "        gene_counts = gene_counts.drop(columns=g)\n",
    "\n",
    "if mutation_feature_mode == 0:\n",
    "    # Simple mutation count per patient: keep only TOTAL_MUTATED_GENES\n",
    "    gene_counts[\"TOTAL_MUTATED_GENES\"] = (gene_counts > 0).sum(axis=1)\n",
    "    # Reduce to single column so no individual gene flags remain\n",
    "    gene_counts = gene_counts[[\"TOTAL_MUTATED_GENES\"]]\n",
    "    print(f\"Feature engineering with mutation count only:\")\n",
    "    print(f\"  - Only TOTAL_MUTATED_GENES retained\")\n",
    "\n",
    "elif mutation_feature_mode == 1:\n",
    "    # Define lethal mutations: top 10 genes by mortality rate from statistically significant mutations\n",
    "    lethal_mutations = [g for g in mutation_df_filtered.head(10).index.tolist() if g not in EXCLUDE_GENES]\n",
    "    print(f\"Lethal mutations selected (statistically significant, p < 0.05) excluding manual list:\")\n",
    "    for gene in lethal_mutations:\n",
    "        row = mutation_df_filtered.loc[gene]\n",
    "        print(f\"  - {gene}: {row['mortality_rate']:.1f}% ({int(row['deaths'])}/{int(row['patient_count'])} patients, p={row['p_value']:.4f})\")\n",
    "    \n",
    "    # Ensure columns for lethal mutations exist and are binary\n",
    "    for gene in lethal_mutations:\n",
    "        if gene in gene_counts.columns:\n",
    "            gene_counts[gene] = (gene_counts[gene] > 0).astype(int)\n",
    "        else:\n",
    "            gene_counts[gene] = 0\n",
    "\n",
    "    # Aggregate all other (non-lethal) genes into OTHER_MUTATIONS_COUNT\n",
    "    existing_cols = [c for c in gene_counts.columns if c not in lethal_mutations]\n",
    "    if existing_cols:\n",
    "        gene_counts[\"OTHER_MUTATIONS_COUNT\"] = gene_counts[existing_cols].sum(axis=1).astype(int)\n",
    "        # Drop the non-lethal individual gene columns\n",
    "        gene_counts = gene_counts.drop(columns=existing_cols)\n",
    "\n",
    "    # Now explicitly keep only lethal flags + OTHER_MUTATIONS_COUNT\n",
    "    final_cols = lethal_mutations.copy()\n",
    "    if 'OTHER_MUTATIONS_COUNT' in gene_counts.columns:\n",
    "        final_cols.append('OTHER_MUTATIONS_COUNT')\n",
    "\n",
    "    # Add TOTAL_MUTATED_GENES as sum of lethal flags + other count\n",
    "    gene_counts[\"TOTAL_MUTATED_GENES\"] = gene_counts[final_cols].sum(axis=1).astype(int)\n",
    "    final_cols.append('TOTAL_MUTATED_GENES')\n",
    "\n",
    "    # Reduce gene_counts to final columns only (enforces no stray gene flags)\n",
    "    gene_counts = gene_counts[final_cols]\n",
    "\n",
    "    print(f\"\\nFeature engineering with lethal mutation flags (strict):\")\n",
    "    print(f\"  - Kept lethal flags: {len(lethal_mutations)}\")\n",
    "    print(f\"  - Kept OTHER_MUTATIONS_COUNT: {'OTHER_MUTATIONS_COUNT' in gene_counts.columns}\")\n",
    "    print(f\"  - Kept TOTAL_MUTATED_GENES: {'TOTAL_MUTATED_GENES' in gene_counts.columns}\")\n",
    "\n",
    "elif mutation_feature_mode == 2:\n",
    "    # Keep all genes as binary indicators\n",
    "    for col in gene_counts.columns:\n",
    "        gene_counts[col] = (gene_counts[col] > 0).astype(int)\n",
    "    \n",
    "    # Adding a column with a total number of mutated genes\n",
    "    gene_counts[\"TOTAL_MUTATED_GENES\"] = (gene_counts > 0).sum(axis=1)\n",
    "    \n",
    "    print(f\"Feature engineering without lethal mutation flags:\")\n",
    "    print(f\"  - All {len(gene_counts.columns) - 1} genes included as binary indicators\")\n",
    "\n",
    "# VAF summary statistics:\n",
    "vaf_stats = molecular.groupby(\"ID\")[\"VAF\"].agg(['mean','max','min']).add_prefix(\"VAF_\")\n",
    "\n",
    "# Combine molecular features\n",
    "mol_features = gene_counts.join(vaf_stats, how=\"left\").fillna(0)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Merge everything into a single training table\n",
    "# -------------------------\n",
    "\n",
    "X = clinical.merge(mol_features, how=\"left\", on=\"ID\").fillna(0)\n",
    "\n",
    "# Remove ID column\n",
    "X = X.set_index(\"ID\")\n",
    "\n",
    "# Drop complex/unwanted clinical columns\n",
    "if 'CYTOGENETICS' in X.columns:\n",
    "    X = X.drop(columns=['CYTOGENETICS'])\n",
    "\n",
    "# Force categorical fields to string (only keep CENTER if present)\n",
    "if 'CENTER' in X.columns:\n",
    "    X[\"CENTER\"] = X[\"CENTER\"].astype(str)\n",
    "\n",
    "# Prepare survival data: need both OS_YEARS (time) and OS_STATUS (event indicator)\n",
    "survival_data = target.set_index(\"ID\").loc[X.index][[\"OS_YEARS\", \"OS_STATUS\"]].copy()\n",
    "survival_data[\"OS_STATUS\"] = survival_data[\"OS_STATUS\"].astype(bool)\n",
    "\n",
    "# Remove patients with missing survival time\n",
    "valid_idx = ~survival_data[\"OS_YEARS\"].isna()\n",
    "X = X[valid_idx]\n",
    "survival_data = survival_data[valid_idx]\n",
    "\n",
    "# Create structured array for survival analysis\n",
    "y = Surv.from_arrays(\n",
    "    event=survival_data[\"OS_STATUS\"].values,\n",
    "    time=survival_data[\"OS_YEARS\"].values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8066f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CENTER  BM_BLAST     WBC   ANC  MONOCYTES    HB    PLT  ABL1  ARID1A  \\\n",
      "ID                                                                             \n",
      "P132697    MSK      14.0    2.80  0.20       0.70   7.6  119.0   0.0     0.0   \n",
      "P132698    MSK       1.0    7.40  2.40       0.10  11.6   42.0   0.0     0.0   \n",
      "P116889    MSK      15.0    3.70  2.10       0.10  14.2   81.0   0.0     0.0   \n",
      "P132699    MSK       1.0    3.90  1.90       0.10   8.9   77.0   0.0     0.0   \n",
      "P132700    MSK       6.0  128.00  9.70       0.90  11.1  195.0   0.0     0.0   \n",
      "...        ...       ...     ...   ...        ...   ...    ...   ...     ...   \n",
      "P121826     VU       1.0    2.50  1.02       0.20  10.2   78.0   0.0     0.0   \n",
      "P121827     VU       1.5    8.10  2.66       0.45  11.3   40.0   0.0     1.0   \n",
      "P121830     VU       0.0    1.80  0.55       0.29   9.4   86.0   0.0     0.0   \n",
      "P121853     VU       5.0    1.37  0.37       0.11  11.4  102.0   0.0     0.0   \n",
      "P121834     VU       0.0    2.70  0.72       0.23   8.2  239.0   0.0     0.0   \n",
      "\n",
      "         ARID2  ...  U2AF1  U2AF2  WHSC1  ZMYM3  ZNF318  ZRSR2  \\\n",
      "ID              ...                                              \n",
      "P132697    0.0  ...    1.0    0.0    0.0    0.0     0.0    0.0   \n",
      "P132698    0.0  ...    0.0    0.0    0.0    0.0     0.0    0.0   \n",
      "P116889    0.0  ...    0.0    0.0    0.0    0.0     0.0    0.0   \n",
      "P132699    1.0  ...    0.0    0.0    0.0    0.0     0.0    1.0   \n",
      "P132700    0.0  ...    0.0    0.0    0.0    0.0     0.0    0.0   \n",
      "...        ...  ...    ...    ...    ...    ...     ...    ...   \n",
      "P121826    0.0  ...    0.0    0.0    0.0    0.0     0.0    0.0   \n",
      "P121827    0.0  ...    0.0    0.0    0.0    0.0     0.0    0.0   \n",
      "P121830    0.0  ...    1.0    0.0    0.0    0.0     0.0    0.0   \n",
      "P121853    0.0  ...    0.0    0.0    0.0    0.0     0.0    0.0   \n",
      "P121834    0.0  ...    0.0    0.0    0.0    0.0     0.0    0.0   \n",
      "\n",
      "         TOTAL_MUTATED_GENES  VAF_mean  VAF_max  VAF_min  \n",
      "ID                                                        \n",
      "P132697                  8.0  0.251578   0.4220   0.0300  \n",
      "P132698                  3.0  0.272867   0.2825   0.2661  \n",
      "P116889                  2.0  0.039333   0.0480   0.0350  \n",
      "P132699                  9.0  0.209227   0.4770   0.0490  \n",
      "P132700                  1.0  0.472100   0.4721   0.4721  \n",
      "...                      ...       ...      ...      ...  \n",
      "P121826                  4.0  0.169000   0.5010   0.0510  \n",
      "P121827                  2.0  0.427500   0.8190   0.0360  \n",
      "P121830                  6.0  0.244650   0.4390   0.0280  \n",
      "P121853                  3.0  0.184250   0.2690   0.0290  \n",
      "P121834                  4.0  0.275220   0.3450   0.0230  \n",
      "\n",
      "[3173 rows x 133 columns]\n"
     ]
    }
   ],
   "source": [
    "# We check the integrity of the merged data\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64241475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. Build preprocessing + model pipeline\n",
    "# -------------------------\n",
    "\n",
    "# Categorical columns from clinical features (compute dynamically in case CYTOGENETICS was dropped)\n",
    "categorical_cols = [c for c in [\"CENTER\", \"CYTOGENETICS\"] if c in X.columns]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# GradientBoostingSurvivalAnalysis - survival-aware model that properly handles censoring\n",
    "gb = GradientBoostingSurvivalAnalysis(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"pre\", preprocess),\n",
    "    (\"model\", gb)\n",
    "])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train/test split and training\n",
    "# -------------------------\n",
    "\n",
    "# Split data (must preserve survival structure)\n",
    "idx_train, idx_test = train_test_split(range(len(X)), test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X.iloc[idx_train]\n",
    "X_test = X.iloc[idx_test]\n",
    "y_train = y[idx_train]\n",
    "y_test = y[idx_test]\n",
    "\n",
    "# Fit the survival pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict risk scores (higher = worse survival)\n",
    "pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81954a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index: 0.7434307854878478\n",
      "IPCW C-index: 0.6995201872194418\n"
     ]
    }
   ],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "\n",
    "# lifelines expects higher scores => longer survival (predicted time),\n",
    "# so invert risk scores for concordance.\n",
    "c_index = concordance_index(\n",
    "    event_times=y_test['time'],\n",
    "    predicted_scores=-pred,               # <- invert\n",
    "    event_observed=y_test['event'].astype(bool)\n",
    ")\n",
    "print(\"C-index:\", c_index)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Evaluating model performance\n",
    "# -------------------------\n",
    "\n",
    "# IPCW C-index accounts for censoring in the evaluation set\n",
    "c_index_ipcw = concordance_index_ipcw(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    pred\n",
    ")[0]   # first entry is the IPCW-c-index\n",
    "\n",
    "print(\"IPCW C-index:\", c_index_ipcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633e48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "============================================================\n",
      "Hyperparameter Optimization Results:\n",
      "============================================================\n",
      "Best parameters found:\n",
      "  model__subsample: 0.9\n",
      "  model__n_estimators: 300\n",
      "  model__min_samples_split: 10\n",
      "  model__min_samples_leaf: 5\n",
      "  model__max_features: sqrt\n",
      "  model__max_depth: 4\n",
      "  model__learning_rate: 0.03\n",
      "Best CV score: 0.7287\n",
      "============================================================\n",
      "============================================================\n",
      "Hyperparameter Optimization Results:\n",
      "============================================================\n",
      "Best parameters found:\n",
      "  model__subsample: 0.9\n",
      "  model__n_estimators: 300\n",
      "  model__min_samples_split: 10\n",
      "  model__min_samples_leaf: 5\n",
      "  model__max_features: sqrt\n",
      "  model__max_depth: 4\n",
      "  model__learning_rate: 0.03\n",
      "Best CV score: 0.7287\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Hyperparameter Optimization\n",
    "# -------------------------\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid for GradientBoostingSurvivalAnalysis\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 300, 600],\n",
    "    'model__learning_rate': [0.01, 0.03, 0.05],\n",
    "    'model__max_depth': [2, 3, 4],\n",
    "    'model__min_samples_split': [5, 10],\n",
    "    'model__min_samples_leaf': [2, 3, 5],\n",
    "    'model__subsample': [0.7, 0.8, 0.9],\n",
    "    'model__max_features': [None, 'sqrt']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with survival-aware scoring\n",
    "# Using negative concordance index for minimization (since GridSearchCV minimizes)\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_concordance_index_ipcw',  # requires custom scorer for survival\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Note: If using standard GridSearchCV without custom survival scorer,\n",
    "# use a simpler parameter grid and manual cross-validation approach:\n",
    "# For now, using RandomizedSearchCV for faster optimization with large grid\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "# Extract best parameters\n",
    "best_params = rs.best_params_\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Hyperparameter Optimization Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"Best CV score: {rs.best_score_:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dac80c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Final Model Performance (with optimized hyperparameters):\n",
      "============================================================\n",
      "C-index: 0.7381\n",
      "IPCW C-index: 0.6908\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7. Train Final Model with Best Hyperparameters\n",
    "# -------------------------\n",
    "\n",
    "# Create final model with best parameters (survival-aware)\n",
    "gb_final = GradientBoostingSurvivalAnalysis(\n",
    "    n_estimators=best_params['model__n_estimators'],\n",
    "    learning_rate=best_params['model__learning_rate'],\n",
    "    max_depth=best_params['model__max_depth'],\n",
    "    min_samples_split=best_params['model__min_samples_split'],\n",
    "    min_samples_leaf=best_params['model__min_samples_leaf'],\n",
    "    subsample=best_params['model__subsample'],\n",
    "    max_features=best_params['model__max_features'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create final pipeline\n",
    "model = Pipeline([\n",
    "    (\"pre\", preprocess),\n",
    "    (\"model\", gb_final)\n",
    "])\n",
    "\n",
    "# Train final model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate final model\n",
    "c_index_final = concordance_index(\n",
    "    event_times=y_test['time'],\n",
    "    predicted_scores=-pred,\n",
    "    event_observed=y_test['event'].astype(bool)\n",
    ")\n",
    "\n",
    "c_index_ipcw_final = concordance_index_ipcw(\n",
    "    y_train,\n",
    "    y_test,\n",
    "    pred\n",
    ")[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Final Model Performance (with optimized hyperparameters):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"C-index: {c_index_final:.4f}\")\n",
    "print(f\"IPCW C-index: {c_index_ipcw_final:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 8. Predict survivability score for all patients (using optimized model)\n",
    "# -------------------------\n",
    "\n",
    "survivability_score = model.predict(X)\n",
    "\n",
    "# Attach score to IDs\n",
    "output = pd.DataFrame({\n",
    "    \"ID\": X.index,\n",
    "    \"SURVIVABILITY_SCORE\": survivability_score\n",
    "})\n",
    "\n",
    "print(output.head())\n",
    "\n",
    "# Save to CSV\n",
    "output.to_csv(\"survivability_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
